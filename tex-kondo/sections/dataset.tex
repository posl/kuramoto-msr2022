\section{Study Design}
\label{sec:design}

% In this section, we describe the data collection process and 
% the overview of the collected dataset. 
This section describes our study design, including research questions, data collection, metrics computation, and data description.

\subsection{Research Questions}
\label{sec:rqs}

To identify the characteristics of the visual issue reports, we addressed the following two research questions. 
\begin{itemize}
	\item[RQ1:] \textbf{\RQone{}}\\
	GitHub argued that visual issue reports easily provide 
	information such as the steps to fix bugs~\citep{github-video-blog}. 
	Hence, we hypothesize that videos reduce the effort of 
	developers to describe such information. 
	In this RQ, we measured the effort in terms of 
	the number of words and 
	study whether this hypothesis is true. 
	\item[RQ2:] \textbf{\RQtwo{}}\\
	We suppose visual issue reports provide 
	more information than the other issue reports. 
	Consequently, developers could discuss the details and 
	result in active discussions. 
	In this RQ, we clarify whether this assumption is true. 
	We measured the activity in terms of 
	the first response time, and
	the number of comments.
	%, and the number of words. 
	\item[RQ3:] \textbf{\RQthree{}}\\
	% We suppose that developers use visualization 
	% in particular issues. 
	% For example, developers may use visualization 
	% to share the way to reproduce bugs. 
	% In this RQ, we clarify the differences 
	% between the issues with and without 
	% visualization. 
	Zimmermann~\et~\citep{zimmermann2010TSE} reported that
	issue reports occasionally have missing or incorrect steps 
	to reproduce bugs, 
	while GitHub argued that visual issue reports easily 
	provide the steps~\citep{github-video-blog}. 
	Hence, we suppose that visual issue reports could be 
	resolved more quickly than the other issue reports 
	because of the sufficient and accurate information. 
	In this RQ, we clarify whether this assumption is true. 
\end{itemize}

\subsection{Context Selection}
To select projects as context for our study, we employed \kashiwa{GitHubSercher???}. GitHubSercher can find repositories satisfying specific criteria. To filter out unpopular, inactive repositories, or repositories that have no issues, we set up the following criteria.
\begin{itemize}
	\item the number of stars $\geq$ 10
	\item the number of issue reports $\geq$ 1
	\item at least one commit was made in 2021
\end{itemize}
Consequently, the number of the repositories satisfying the criteria was 289,115. From November 2021 to December 2021, we collected 770,656 closed issue reports from 4,173 projects that were randomly selected. While the number of sampled projects seems odds, we collected all the closed issue reports from as many projects as possible in the limited time.  
As the sampled projects accounts for only less than 2\% of all repositories, we discuss the threat to validity of this process in \sec{sec:limitation}. 


% \input{figures/data-collection-overview}

\subsection{Data Collection}
%\fig{fig:data-collection-overview} shows an overview of the data collection process. 
We first collected closed issue reports with \texttt{PyGitHub}\footnote{\url{https://pygithub.readthedocs.io/en/latest/index.html}} that internally execute GitHub API v3\footnote{\url{https://docs.github.com/en/rest}}. 

Next, we collected videos and images attached to the issue reports. While GitHub users can see videos and images on issue pages, the videos and images are stored in different URLs. As the URLs are written in the text description of issue reports, we parsed them with regular expressions and downloaded them. The regular expressions we used are shown as follows:
\begin{quote}
\addtolength\leftmargini{0in}
{\it https://user-images.githubusercontent.com/[a-zA-Z0-9\textbackslash-/]+\textbackslash.[a-zA-Z0-9]+}
\end{quote}
Each downloaded file was determined to be a image or video by its extension. Specifically, png, PNG, jpg, JPG, and jpeg are treated as images, and  gif, GIF, mp4, MP4, and mov as videos.
Consequently, we downloaded 33,079 images and 3,819 movies
with the collected URLs.\masa{to kuramoto: i wrote these numbers. are the numbers correct?} 

\input{tables/issue-category}
We classified the issue reports into three categories based on whether they have images and movies. \tab{tab:issue-category} shows the number of issue reports for each category. 
Note that issue reports often have both images and movies. 
These issue reports are included in both $Img$ and $Mov$ categories. 
Thus, the total number of downloaded issue reports is different from 
the sum of \#issues in \masa{Table X}\masa{Kashiwa-sensei wrote this right? I highlight the unknown table number->Yes}. 
In this paper, we refer to the issue reports in the $Mov$ category 
as \textit{visual issue reports}. 


% \input{tables/issue-ex}
% We extract a part of the retrieved issues in \tab{tab:example-dataset}. 
% Each row corresponds to the values of the attributes of an issue. 
%\input{tex-kondo/tables/issue-attr}
\input{./tables/issue-attr}
\subsection{Analysis}
We retrieved the attributes from the collected issue reports.
\tab{tab:issue-attr} shows seven attributes extracted from 
the issue reports. 
The attributes can be classified into three dimensions, 
``Report'', ``Discussion'', and ``Fix''. 
The attributes in dimension ``Report'' are extracted from 
the description of issue reports or attached files 
when the issue was created. 
In particular, in RQ1, we 
%calculate 
utilize the number of words 
in reports 
%(\ie, $\#comments$) 
(\ie, $\#words$) \masa{not \#comments right?}
for Img, Vid, and None. 
In addition, 
$IssueCreatedYear$, $\#imgs$, and $\#vids$ are used to show 
dataset description. 
Note that these attributes are not calculated from either title, not comments (i.e., only descriptions were used). 
Also, when computing $\#words$, if the description of the issue report 
includes URLs for images/videos, 
we exclude them from the description because these are not words.

Dimension ``Discussion''
has two attributes $\#comments$ 
which shows the number of comments for each issue report and 
$FirstCommentTime$ which shows the time from 
reported to comment at the first time.
We utilize these attributes as proxies of the level of 
discussion activities in RQ2.
\kashiwa{TODO: ADD Explanations about comments and FirstComment time}
\masa{how about this?}

Dimension ``Fix'' has one attribute $IssueResolvedTime$ which shows the time from reported to fixed. 
Note that some negative values of $IssueResolvedTime$ were observed. We investigated it manually and found that these are because of a bug in GitHub.  We excluded those issues that have negative values from our dataset. 
In addition, we excluded issue reports resolved in too short or long periods (e.g., 30 seconds) because they might be issued after being fixed or might not be fixed in reality.
%\kashiwa{check please}
Specifically, we only use the issue reports that meet the condition: $30\ sec \leq IssueResolvedTime \leq 1\ year$.
The number of issue reports that meet this condition is 711,160 (92.23\%).


\kashiwa{WAKARANAI???KOKOKARA}
%On the contrary, $\#chars$, $\#imgs$, $\#movs$, and $Words$ need several pre-processes from issue reports.  We counted the number of characters as $\#chars$ and stored the words as $Words$ for all issue reports. In this process, if the description of the issue report includes the URL, we exclude it from the description, and convert all issue reports into $\#chars$ and $Words$.
%We counted the appearances of images and movies as $\#imgs$ and $\#movs$ with the regular expression for the URL.
\kashiwa{???KOKOMADE}
\masa{I removed this and extended the explanation of the Report dimension}



\kashiwa{Write what do you analyze (e.g., compare them in median)}
To evaluate the difference, we used a non-parametric test \textit{Steel-Dwass test}.
\kashiwa{Write what this test is}
\kashiwa{Write why this test is selected}
because our preliminary study shows that 
the distributions for each category do not 
come from normal distributions.
\kashiwa{Write the reason why correction is not needed (i.e., how to deal with family-wise error rate)}