\section{Study Design}
\label{sec:design}

% In this section, we describe the data collection process and 
% the overview of the collected dataset. 
This section describes our study design, including research questions, data collection, metrics computation, and data description.

\subsection{Research Questions}
\label{sec:rqs}

To identify the characteristics of the visual issue reports, we addressed the following three research questions, focusing on Report (RQ1), Discussion (RQ2), and Fix (RQ3). 
\begin{itemize}
	\item[RQ1:] \textbf{\RQone{}}\\
	Developers often suffer from reproducing bugs with the reported information~\citep{DBLP:conf/sigsoft/ChaparroLZMPMBN17}\citep{DBLP:conf/icsm/0001KC20}\citep{zimmermann2010TSE}. 
	On the other hand, as reporters are not always developers, it is not easy to tell what they did and what they encount~\citep{DBLP:conf/sigsoft/ChaparroBLMMPPN19}. Thus, GitHub developed a function that can easily provide information with videos and officially announced the function release on May, 2021~\citep{github-video-blog}. Potter and Faulconer~\citep{POTTER1975} showed that visual images in general are a more effective approach to describe what people want to communicate and let other people understand it, compared with texts. We hypothesize that videos can reduce the effort for reporting bugs. In this RQ, we measure the number of words in the description of issues as a proxy measure for the effort. 
	\item[RQ2:] \textbf{\RQtwo{}}\\
	Joorabchi \et~\citep{DBLP:conf/msr/JoorabchiMM14} show that lack of proper communication between reporters and developers end up with being a cause of non-reproducible bug reports. In addition, many studies claim that comments made to a bug contribute to improving bug-fixing activities~\citep{DBLP:conf/icse/GigerPG10}\citep{DBLP:conf/msr/Panjer07}\citep{zhang2012WCRE}.  Visual issue reports might have the potential to attract developers and receive many comments from the developers. In this RQ, we examine the number of comments in the closed issues and the days to receive the first comment.
	% to clarify whether visual can attract developers これは突っ込まれる可能性が高いのでやめとく． 
	\item[RQ3:] \textbf{\RQthree{}}\\
	% We suppose that developers use visualization 
	% in particular issues. 
	% For example, developers may use visualization 
	% to share the way to reproduce bugs. 
	% In this RQ, we clarify the differences 
	% between the issues with and without 
	% visualization. 
	Zimmermann~\et~\citep{zimmermann2010TSE} reported that issue reports occasionally have missing or incorrect steps to reproduce bugs, which delays the entire bug-fixing process~\citep{github-video-blog}. Also, Ohira et al. ~\citep{DBLP:conf/icsm/OhiraHOM12} shows that bug-fixing activities delays when the reporter and developer are different persons because they require communications. Visual issues may mitigate this issue by facilitating their communication. In this RQ, we measure the time from reported to closed to evaluate how quickly visual issue reports are resolved, compared with issues without videos or images. 
\end{itemize}

\subsection{Context Selection}
\label{sec:design:context}
To select projects as context for our study, we employed \texttt{GitHub Search}~\citep{msr2021data}. GitHub Search can find repositories satisfying specific criteria. To filter out unpopular, inactive repositories, or repositories that have no issues, we set up the following criteria.
\begin{itemize}
	\item the number of stars $\geq$ 10
	\item the number of issue reports $\geq$ 1
	\item at least one commit was made in 2021
% 	\item not include the following tags\\invalid, duplicate, document, question and enhancement
\end{itemize}
Consequently, the number of the repositories satisfying the criteria was 289,115. From November 2021 to December 2021, we collected 263,596 closed issue reports from 4,173 projects that were randomly selected. 
We collected all the closed issue reports from as many projects as possible in the limited time.  



% \input{figures/data-collection-overview}

\subsection{Data Collection}
%\fig{fig:data-collection-overview} shows an overview of the data collection process. 
We first collected closed issue reports with the method \texttt{get\_issue} provided by  \texttt{PyGitHub}\footnote{\url{https://pygithub.readthedocs.io/en/latest/index.html}} that internally execute GitHub API v3.\footnote{\url{https://docs.github.com/en/rest}} In total, we collected \textcolor{red}{XXX} closed issue reports. 

Next, we collected videos and images attached to the issue reports. While GitHub users can see videos and images on issue pages, the videos and images are stored in different URLs. As the URLs are written in the text description of issue reports, we parsed them with regular expressions and downloaded them. The regular expressions we used are shown as follows:
\begin{quote}
\addtolength\leftmargini{0in}
{\it https://user-images.githubusercontent.com/[a-zA-Z0-9\textbackslash-/]+\textbackslash.[a-zA-Z0-9]+}
\end{quote}
Each downloaded file was determined by its extension to be an image, a video, or neither of these. We used only images and videos. Specifically, ``png'', ``PNG'', ``jpg'', ``JPG'', and ``jpeg'' are treated as images, and  ``gif'', ``GIF'', ``mp4'', ``MP4'', and ``mov'' as videos. 
Consequently, we downloaded \textcolor{red}{XXX} images and \textcolor{red}{XXX} videos with the collected URLs. 
%These issues, images, and movies are also included in the dataset (i.e., original dataset) we provide. 

Then, we removed issue reports inappropriate for this study. As \texttt{get\_issue} method collect pull requests as well, we excluded pull requests from the original dataset (\textcolor{red}{XXX} issues).  
In addition, different from Bugzilla~\citep{Bugzilla} or Jira~\citep{JIRA}, GitHub issues do not have resolution statuses (e.g., ``FIXED'', ``DUPLICATED''). 
Instead,  GitHub provides default tags to indicate these resolution statuses. 
We excluded \textcolor{red}{XXX} issues with tags indicating invalid issues (i.e., ``duplicated'', ``invalid'') or tags indicating non-bug (i.e., ``document'', ``question'', "enhancement"). Also, we removed \textcolor{red}{XXX} issue reports resolved in too short ($\leq$ 30 seconds) or long periods  ($\geq$ one year) because developers leave bugs for long years without addressing or they report issues after bug-fix. 

\input{tables/issue-category}
{\bf Dataset summary.} The final dataset contains 263,596 issue reports,  21,003 images, and 1,324 movies. 
These issue reports are classified into three categories based on whether they have either image(s) or video(s). 
\tab{tab:issue-category} shows the number of issue reports for each category. 
Note that issue reports often have both images and videos. 
These issue reports are counted in both $Img$ and $Vid$ categories (only around 0.05\%). 
Thus, the total number of downloaded issue reports (i.e., 263,596) is different from the sum of issues (i.e., 263,725).
In this paper, we refer to the issue reports in the $Img$ and $Vid$ categories
as \textit{visual issue reports}. 

\input{figures/data-category-trend}
Out of the collected issues, only 7.97\% of issue reports have images, and 0.50\% have videos. 
While this number seems to be small, looking into the trend shown in \fig{fig:data-cat-trend}, the rate of visual issue reports by year is increasing from 2017 to 2021. 
The ratio of visual issue reports reached to 13\% between 2017 and 2021. 
Also, we found that GitHub officially launched the function to share videos in May 2021 but developers often had uploaded videos before the beta release of the function~\citep{github-video-blog-beta}. 
When we manually looked into issue reports, GitHub had allowed users to attach GIF files on the descriptions.

% \input{tables/issue-ex}
% We extract a part of the retrieved issues in \tab{tab:example-dataset}. 
% Each row corresponds to the values of the attributes of an issue. 
%\input{tex-kondo/tables/issue-attr}
\input{./tables/issue-attr}
\subsection{Analysis}
We retrived attributes from the collected issue reports. \tab{tab:issue-attr} shows six attributes used in this study, which are classified into three dimensions, ``Report'', ``Discussion'', and ``Fix''. 

The attributes in the dimension ``Report'' are extracted from the description of issue reports or attached files when the issue was created. 
In particular, in RQ1, we count the number of words in reports (\ie, $DescriptionLength$) for Img, Vid, and None. In addition,  $\#imgs$ and $\#vids$ are used to show the average number of the files attached in reports. 
Note that these attributes are not calculated from either title, not comments (i.e., only descriptions were used). Also, URLs in the descriptions to attach images/videos are not counted as words in $DescriptionLength$.

The dimension ``Discussion'' has two attributes, $Comments$ and $FirstCommentTime$. 
$Comments$ is the number of comments that were made to an issue report. 
We utilize this attribute as a proxy measure of discussion effort. $FirstCommentTime$ is the time difference in days between when the first comment was made and when the issue was reported. 
We use this attribute for measuring developers' interest. 

The dimension ``Fix'' has $IssueResolvedTime$ which is the time difference in days between when the issue was closed and when the issue was reported. 

For each research question, we measure the median values of the attributes to compare $Img$, $Vid$ and $None$. 
Also we apply a non-parametric test \textit{Steel-Dwass test}~\citep{steel-dwass-test} to evaluate the difference. 
\textit{Steel-Dwass test} performs the multiple comparisons while taking into account the number of comparisons to prevent increases in the family-wise error rate.
The datasets do not follow normal distributions, and do not satisfy homoscedasticity, and therefore are good candidates for analysis using the Steel-Dwass test. 
