\section{Discussion}
%\input{tex-kondo/tables/tf-idf-result}
\subsection{Why Do Issue Reports with Videos or Images Take a Longer Time than Others?}
In this study, we observed that developers still write as many words in visual issue reports as in non-visual issue reports (RQ1). 
We also found that visual issue reports receive more comments that non-visual issue reports (RQ2). 
However, we confirmed that developers consume time to close visual issues reports longer than non-visual issues (RQ3), which rejected our hypothesis. 
To investigate the reason why visual issue reports take a longer time to be clolsed, we examine the differences in the contents of bugs in this section. 

We extracted words from the descriptions of closed issue reports in the dataset, and  removed stop words such as ``at'', ``it'', and ``the'' from them. 
Then, we calculated TF-IDF values to clarify the characteristic words for each types of issues (i.g., Vid, Img, None). 

\tab{tab:tfidf-result} shows the top-10 characteristic words in each category, calculated by TF-IDF.
First, in the $Vid$ categories, we observed many words  related to GUI such as ``dropdown'' and ``button''. It is worth noting that ``when'' is located at the top rank of the lists in both $Img$ and $Vid$. 
This implies that reporters may explain steps to reproduce complicated problems. 

On the other hand, ``dependabot'' and ``pull request'' are located at the top of  $None$.\footnote{closed issue reports in our dataset does not include pull requests.\kashiwa{check please}} 
This might be that suggested changes had been prepared when the issue is created. 
Future research should take account into creating the dataset of non-visual issue reports that contain steps to reproduce. 

\input{./tables/tf-idf-result}

\subsection{Future Research Direction}
This section discusses what should be considered by future studies. 

\noindent
\textbf{Fine-grained analysis. }
In RQ2, we showed that visual issue reports are 3-times likely to receive more comments than non-visual issues. As these might be caused by that  issue reports attract many developers or that visual issue reports deal with difficult problems. Future studies should examine how many developers are involved~\citep{DBLP:conf/icsm/BavotaR15}, severity tags~\citep{DBLP:conf/issre/ZhouNG15}, and size of changes~\citep{DBLP:conf/kbse/HattoriL08}.

In this study, we studied only closed bugs and examined only resolution time in RQ3 (Fix). However, previous studies examined several statuses of bugs. For example, Joorabchi et al.~\citep{DBLP:conf/msr/JoorabchiMM14} studied ``Works For Me'', Shihab et al.~\citep{DBLP:journals/ese/ShihabIKIOAHM13} studied Reopen bugs, and Zou~\et~\citep{DBLP:conf/compsac/ZouXZCL15} examined bug fixing rate (\eg, ``Won't fix''). However, most of the studies use other bug tracking systems, Bugzilla~\citep{Bugzilla} or Jira~\citep{JIRA}. These bug tracking systems have various resolution statuses such as ``Won't Fix'' and ``Works For Me''  in default but GitHub we studied does not. Developers on GitHub can provide tags but it is not common (\masa{X}\%). Future studies should collect more issues and show the percentage of each status, etc. 

\noindent
\textbf{Bug reproduction Automation.}
Developers often suffer from reproducing bugs with the reported information~\citep{DBLP:conf/sigsoft/ChaparroLZMPMBN17}\citep{DBLP:conf/icsm/0001KC20}\citep{zimmermann2010TSE}.
Automating this process would support developers to quickly find and fix the cause of bugs. 
Our final goal of this study is to automate bug reproduction. 
We believe that we can make use of image processing technique ~\citep{DBLP:conf/nips/KrizhevskySH12}, using the uploaded videos, in order to identify which pages/screens of systems was being used and what actions was done by users (e.g., which button was clicked). 
This approach would reduce efforts for evaluating if reported issues can be reproducable. 
\kashiwa{To Masa: Please check if the citation is appropriate for the modified sentenses. }


%Kashiwa: I commented out the following things because this is a bit small topic. I want to talk about future direction here. 

% \textbf{Improve the quality of the dataset.}
% Our dataset has space to further improve.
% First, removing the bot comments as much as 
% possible is necessary. 
% Almost all OSSs use bots to promote software development 
% such as automatically closing the abandoned issues with 
% a comment. 
% However, such bots may change the characteristics of 
% the dataset. 
% Also, classifying the data based on 
% the characteristics of the repositories is important. 
% Currently, we collected all repositories as one dataset. 
% However, the repositories have their characteristics. 
% A repository is a web framework while another repository is 
% a machine learning library. 
% In addition, the development state of repositories is different. 
% The characteristics of the repositories in which 
% developers start to develop and the repositories that 
% have already released many major versions should be different. 
% Finally, collecting all repositories that meet 
% the condition described in \sec{sec:dataset} is necessary. 


% \noindent
% \textbf{The impact analysis of movies and images on software development.}
% GitHub added the feature~\citep{github-video-blog} to easily 
% share movies with GitHub to earn advantages 
% in software development such as reproducing bugs easily in issues and 
% explaining the background of changes in pull requests. 
% However, no studies exist that investigate whether such movies 
% impact software development. 
% Hence, investigating the impact of movies on software development 
% is a future research direction. 
%  

